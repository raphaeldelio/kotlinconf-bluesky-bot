{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Analysis with AI\n",
    "\n",
    "In this notebook, we'll analyze the enriched events from the previous notebooks. We'll use a combination of AI techniques to analyze the data and build a simple question-answering system.\n",
    "\n",
    "## Overview of Previous Notebooks\n",
    "\n",
    "In the previous notebooks, we've built a pipeline for processing Bluesky posts:\n",
    "\n",
    "1. **JetStream Consumer**: We consumed Bluesky's Jetstream Websocket and inserted events into a Redis Stream.\n",
    "2. **JetStream Filtering**: We filtered events using Redis Bloom Filter for deduplication and a machine learning model for content-based filtering.\n",
    "3. **Events Enrichment**: We enriched the filtered events with topic modeling and embeddings for semantic search.\n",
    "\n",
    "## What We'll Build in This Notebook\n",
    "\n",
    "In this notebook, we'll build a simple question-answering system that can:\n",
    "\n",
    "1. Identify trending topics in the posts\n",
    "2. Perform semantic search to understand user queries\n",
    "3. Summarize posts about specific topics using a Large Language Model (LLM)\n",
    "4. Route different types of user queries to the appropriate handler\n",
    "\n",
    "This demonstrates how to combine Redis, vector search, and LLMs to build an intelligent data analysis system."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment. We'll need:\n",
    "\n",
    "1. Helper functions from previous notebooks\n",
    "2. Ktor client for HTTP requests\n",
    "3. Serialization for JSON parsing\n",
    "4. Coroutines for asynchronous programming\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:20.998775Z",
     "start_time": "2025-05-22T18:12:20.832277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%use ktor-client\n",
    "%use serialization\n",
    "%use coroutines"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:21.027656Z",
     "start_time": "2025-05-22T18:12:21.003455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import redis.clients.jedis.JedisPooled\n",
    "\n",
    "val jedisPooled = JedisPooled()"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setting Up the Ollama API Client\n",
    "We'll use the Spring AI Ollama client to interact with the Ollama API.\n",
    "\n",
    "Ollama is a tool that allows us to run large language models locally."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:21.064247Z",
     "start_time": "2025-05-22T18:12:21.032512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.ollama.OllamaChatModel\n",
    "import org.springframework.ai.ollama.api.OllamaApi\n",
    "import org.springframework.ai.ollama.api.OllamaApi.ChatRequest\n",
    "import org.springframework.ai.ollama.api.OllamaApi.Message\n",
    "import org.springframework.ai.ollama.api.OllamaApi.Message.Role\n",
    "import org.springframework.ai.ollama.api.OllamaOptions\n",
    "\n",
    "val ollamaApi = OllamaApi.builder()\n",
    "    .baseUrl(\"http://localhost:11434\")\n",
    "    .build()\n",
    "\n",
    "val ollamaOptions = OllamaOptions.builder().model(\"deepseek-coder-v2\").build()\n",
    "\n",
    "val ollamaChatModel = OllamaChatModel.builder()\n",
    "    .ollamaApi(ollamaApi)\n",
    "    .defaultOptions(ollamaOptions)\n",
    "    .build()"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating Post Index"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import redis.clients.jedis.search.IndexDefinition\n",
    "import redis.clients.jedis.search.IndexOptions\n",
    "import redis.clients.jedis.search.Schema\n",
    "import redis.clients.jedis.search.schemafields.VectorField.VectorAlgorithm\n",
    "\n",
    "val schema = Schema()\n",
    "    .addTextField(\"parentUri\", 1.0)\n",
    "    .addTagField(\"topics\", \"|\")\n",
    "    .addTextField(\"time_us\", 1.0)\n",
    "    .addTagField(\"langs\")\n",
    "    .addTextField(\"uri\", 1.0)\n",
    "    .addTagField(\"operation\")\n",
    "    .addTagField(\"did\")\n",
    "    .addNumericField(\"timeUs\")\n",
    "    .addTagField(\"rkey\")\n",
    "    .addTextField(\"rootUri\", 1.0)\n",
    "    .addTextField(\"text\", 1.0)\n",
    "\n",
    "// Define index options (e.g., prefix)\n",
    "val rule = IndexDefinition()\n",
    "    .setPrefixes(\"post:\")\n",
    "\n",
    "// Create the index\n",
    "try {\n",
    "    jedisPooled.ftCreate(\"postIdx\", IndexOptions.defaultOptions().setDefinition(rule), schema)\n",
    "} catch (e: JedisDataException) {\n",
    "    println(\"Index already exists\")\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating a Topic Modeling Function\n",
    "\n",
    "This function takes a post as input and uses the Ollama API to extract topics from the post . The function returns a string of comma -separated topics .\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:13:58.626909Z",
     "start_time": "2025-05-22T18:13:58.579842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dev.raphaeldelio.topicModelingSystemPrompt\n",
    "import org.springframework.ai.chat.messages.SystemMessage\n",
    "import org.springframework.ai.chat.messages.UserMessage\n",
    "import org.springframework.ai.chat.prompt.Prompt\n",
    "\n",
    "fun extractTopics(post: String, existingTopics: String): String {\n",
    "    // Build a chat message\n",
    "    val messages = listOf(\n",
    "        SystemMessage(topicModelingSystemPrompt),\n",
    "        UserMessage(\"Existing topics: $existingTopics\"),\n",
    "        UserMessage(\"Post: $post\")\n",
    "    )\n",
    "\n",
    "    val response = ollamaChatModel.call(Prompt(messages))\n",
    "    return response.result.output.text ?: \"\"\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Semantic Routing with Vector Search\n",
    "\n",
    "To build our question-answering system, we need to understand what the user is asking. We'll use a technique called semantic routing to classify user queries into different categories.\n",
    "\n",
    "For example, if a user asks \"What's trending right now?\", we want to route this to our trending topics handler. If they ask \"What are people saying about Trump?\", we want to route this to our summarization handler.\n",
    "\n",
    "We'll use vector search to match user queries to predefined routes. First, let's define some example queries for the trending topics route:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:21.096510Z",
     "start_time": "2025-05-22T18:12:21.068412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val trendingTopicsRoute = listOf(\n",
    "    \"What are the most mentioned topics?\",\n",
    "    \"What's trending right now?\",\n",
    "    \"What’s hot in the network\",\n",
    "    \"Top topics?\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setting Up the Vector Store\n",
    "\n",
    "To implement our query routing, we'll use Redis as a vector store to store and search for similar queries. A vector store is a database that stores vector embeddings and allows for efficient similarity search.\n",
    "\n",
    "We'll use Redis as our vector store and Spring AI to create embeddings. First, let's set up the embedding model:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:21.315796Z",
     "start_time": "2025-05-22T18:12:21.101271Z"
    }
   },
   "cell_type": "code",
   "source": "@file:DependsOn(\"org.springframework.ai:spring-ai-redis-store:1.0.0-RC1\")",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:23.513371Z",
     "start_time": "2025-05-22T18:12:21.318908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.transformers.TransformersEmbeddingModel\n",
    "\n",
    "val embeddingModel = TransformersEmbeddingModel()\n",
    "embeddingModel.setModelResource(\"file:../resources/model/bge-large-en-v1.5/model.onnx\")\n",
    "embeddingModel.setTokenizerResource(\"file:../resources/model/bge-large-en-v1.5/tokenizer.json\")\n",
    "embeddingModel.afterPropertiesSet()"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuring the Redis Vector Store\n",
    "\n",
    "Now, let's configure the Redis vector store. We'll use Spring AI's RedisVectorStore, which provides a high-level interface for storing and searching vector embeddings in Redis.\n",
    "\n",
    "The configuration includes:\n",
    "- The index name for our vector store\n",
    "- The field names for content and embeddings\n",
    "- Metadata fields for storing additional information\n",
    "- The prefix for our keys in Redis\n",
    "- The vector algorithm to use for similarity search (FLAT in this case)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:23.626790Z",
     "start_time": "2025-05-22T18:12:23.518652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.vectorstore.redis.RedisVectorStore\n",
    "import org.springframework.ai.vectorstore.redis.RedisVectorStore.MetadataField\n",
    "import redis.clients.jedis.search.Schema.FieldType\n",
    "\n",
    "val redisVectorStore = RedisVectorStore.builder(jedisPooled, embeddingModel)\n",
    "    .indexName(\"routeIdx\")\n",
    "    .contentFieldName(\"text\")\n",
    "    .embeddingFieldName(\"textEmbedding\")\n",
    "    .metadataFields(\n",
    "        MetadataField(\"route\", FieldType.TEXT),\n",
    "        MetadataField(\"minThreshold\", FieldType.NUMERIC),\n",
    "    )\n",
    "    .prefix(\"route:\")\n",
    "    .initializeSchema(true)\n",
    "    .vectorAlgorithm(RedisVectorStore.Algorithm.FLAT)\n",
    "    .build()\n",
    "redisVectorStore.afterPropertiesSet()"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating and Storing Route Documents\n",
    "\n",
    "Now that we have our vector store set up, we need to create documents for our routes and store them in the vector store. Each document represents a possible user query and contains:\n",
    "\n",
    "- The route it belongs to (e.g., \"trending_topics\")\n",
    "- The text of the query (e.g., \"What's trending right now?\")\n",
    "- A minimum threshold for matching (to avoid false positives)\n",
    "\n",
    "We'll create a function to create these documents and another function to store them in Redis:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:23.792436Z",
     "start_time": "2025-05-22T18:12:23.630767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.document.Document\n",
    "import java.util.UUID\n",
    "\n",
    "fun createRouteDocument(route: String, text: String, minThreshold: Double): Document {\n",
    "    return Document(\n",
    "        UUID.randomUUID().toString(),\n",
    "        text,\n",
    "        mapOf(\n",
    "            \"route\" to route,\n",
    "            \"text\" to text,\n",
    "            \"minThreshold\" to minThreshold,\n",
    "        )\n",
    "    )\n",
    "}\n",
    "\n",
    "fun storeRouteDocumentsInRedis(routeName: String, minThreshold: Double, routeSamples: List<String>) {\n",
    "    val trendingTopicDocuments = routeSamples.map { text ->\n",
    "        createRouteDocument(routeName, text, minThreshold)\n",
    "    }\n",
    "\n",
    "    redisVectorStore.add(trendingTopicDocuments)\n",
    "}\n",
    "\n",
    "storeRouteDocumentsInRedis(\"trending_topics\", 0.9, trendingTopicsRoute)"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing Vector Search\n",
    "\n",
    "Let's test our vector store by searching for a query similar to the ones we've stored. We'll use the `similaritySearch` method to find the most similar document to our query:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:24.020328Z",
     "start_time": "2025-05-22T18:12:23.798261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.vectorstore.SearchRequest\n",
    "\n",
    "redisVectorStore.similaritySearch(\n",
    "    SearchRequest.builder()\n",
    "        .topK(1)\n",
    "        .query(\"Hey Dev Bubble. What's trending today? Excited to hear the news!\")\n",
    "        .build()\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document{id='42590f13-44d0-43c4-afe6-98c98b25a3fc', text='What's trending right now?', media='null', metadata={vector_score=0.09470153, minThreshold=0.9, route=trending_topics, distance=0.09470153}, score=0.9052984714508057}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Route Matching\n",
    "\n",
    "Now that we have our vector store set up and tested, we need to create a function to match user queries to routes. This function will:\n",
    "\n",
    "1. Break the user query into clauses (to handle complex queries)\n",
    "2. For each clause, find the most similar document in our vector store\n",
    "3. Check if the similarity score is above the minimum threshold\n",
    "4. Return the set of matched routes\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:24.269670Z",
     "start_time": "2025-05-22T18:12:24.030187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import redis.clients.jedis.search.FTSearchParams\n",
    "import redis.clients.jedis.search.Query\n",
    "\n",
    "fun breakSentenceIntoClauses(sentence: String): List<String> {\n",
    "    return sentence.split(Regex(\"\"\"[!?,.:;()\"\\[\\]{}]+\"\"\"))\n",
    "        .filter { it.isNotBlank() }.map { it.trim() }\n",
    "}\n",
    "\n",
    "fun matchRoute(query: String): Set<String> {\n",
    "    return breakSentenceIntoClauses(query).flatMap { clause ->\n",
    "        val result = redisVectorStore.similaritySearch(\n",
    "            SearchRequest.builder()\n",
    "                .topK(2)\n",
    "                .query(clause)\n",
    "                .build()\n",
    "        )\n",
    "\n",
    "        val route = result?.firstOrNull()?.metadata?.get(\"route\") as String\n",
    "        val minThreshold = result.firstOrNull()?.metadata?.get(\"minThreshold\") as String\n",
    "\n",
    "        result.forEach {\n",
    "            println(clause)\n",
    "            println(route)\n",
    "            println(it.score ?: 0.0)\n",
    "            println(minThreshold)\n",
    "            println()\n",
    "        }\n",
    "\n",
    "        result.filter { (it?.score ?: 0.0) > minThreshold.toDouble() }.map {\n",
    "            it?.metadata?.get(\"route\") as String\n",
    "        }\n",
    "    }.toSet()\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing Route Matching\n",
    "\n",
    "Let's test our route matching function with a sample query:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:24.735719Z",
     "start_time": "2025-05-22T18:12:24.278383Z"
    }
   },
   "cell_type": "code",
   "source": "matchRoute(\"Hey DevBubble, what's trending today? Excited to hear the news!\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey DevBubble\n",
      "trending_topics\n",
      "0.7936378717422485\n",
      "0.9\n",
      "\n",
      "Hey DevBubble\n",
      "trending_topics\n",
      "0.7925964593887329\n",
      "0.9\n",
      "\n",
      "what's trending today\n",
      "trending_topics\n",
      "0.979021430015564\n",
      "0.9\n",
      "\n",
      "what's trending today\n",
      "trending_topics\n",
      "0.8581466674804688\n",
      "0.9\n",
      "\n",
      "Excited to hear the news\n",
      "trending_topics\n",
      "0.8192508816719055\n",
      "0.9\n",
      "\n",
      "Excited to hear the news\n",
      "trending_topics\n",
      "0.8015573024749756\n",
      "0.9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[trending_topics]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementing Trending Topics\n",
    "\n",
    "Now that we have our route matching function, let's implement the trending topics handler. This handler will:\n",
    "\n",
    "1. Get the current minute (to query the count-min sketch for the current time window)\n",
    "2. Get all topics ever added to Redis\n",
    "3. For each topic, get the count from the count-min sketch\n",
    "4. Sort the topics by count (descending)\n",
    "5. Take the top 10 topics\n",
    "6. Return them as a set\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:24.846615Z",
     "start_time": "2025-05-22T18:12:24.741373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.chat.messages.SystemMessage\n",
    "import org.springframework.ai.chat.messages.UserMessage\n",
    "import org.springframework.ai.chat.prompt.Prompt\n",
    "import java.time.LocalDateTime\n",
    "\n",
    "fun trendingTopics(): Set<String> {\n",
    "    val currentMinute = LocalDateTime.now().withMinute(0).withSecond(0).withNano(0).toString()\n",
    "    val topTopics = jedisPooled.topkList(\"topics-topk:$currentMinute\")\n",
    "    topTopics.add(\"These are the most mentioned topics. Don't try to guess what's being said in the topics.\")\n",
    "    return topTopics.filter { it.isNotBlank() }.toSet()\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing Trending Topics\n",
    "\n",
    "Let's test our trending topics function:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:24.898521Z",
     "start_time": "2025-05-22T18:12:24.854030Z"
    }
   },
   "cell_type": "code",
   "source": "trendingTopics()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AI Tooling, AI Ethics, Mental Health, Generative Models, Emotional Intelligence, Open-Source Projects, Crypto, AI Automation, AI in Healthcare, DeFi, Anthropic, Video Creation, Content Generation, Multimodal Learning, Claude, These are the most mentioned topics. Don't try to guess what's being said in the topics.]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating a Trending Topics Handler\n",
    "\n",
    "Now that we have our trending topics function, let's create a handler that can be used by our query router. This handler will:\n",
    "\n",
    "1. Take a route and a query as input\n",
    "2. If the route is \"trending_topics\", call our trendingTopics function\n",
    "3. Otherwise, return an empty list\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:24.970938Z",
     "start_time": "2025-05-22T18:12:24.905533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val trendingTopicsHandler: (String, String) -> Iterable<String> = { route, query ->\n",
    "    when (route) {\n",
    "        \"trending_topics\" -> trendingTopics()\n",
    "        else -> emptyList()\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Processing User Requests\n",
    "\n",
    "Now that we have our trending topics handler, let's create a function to process user requests. This function will:\n",
    "\n",
    "1. Take a user query and a handler function as input\n",
    "2. Use our matchRoute function to determine which routes match the query\n",
    "3. Call the handler function for each matched route to get the relevant data\n",
    "4. Use a Large Language Model to generate a response based on the user query and the data\n",
    "\n",
    "The LLM will help us generate a natural language response that summarizes the data in a concise way.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:19:34.749465Z",
     "start_time": "2025-05-22T18:19:34.682823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fun processUserRequest(\n",
    "    query: String,\n",
    "    handler: (String, String) -> Iterable<String>\n",
    "): String {\n",
    "    val routes = matchRoute(query)\n",
    "    println(routes)\n",
    "\n",
    "    if (routes.isEmpty()) {\n",
    "        return \"Sorry, I couldn't find any relevant information from your post. Try asking what's trending or what people are saying about a specific topic.\"\n",
    "    }\n",
    "\n",
    "    val enrichedData = routes.map { route -> handler(route, query) }\n",
    "    println(enrichedData + \"\\n\")\n",
    "\n",
    "    val systemPrompt = \"You are a bot that helps users analyse posts about artificial intelligence posts. You may be given a data set to help you answer questions. Answer in a max od 300 chars. I MEAN IT. It's a TWEET. Don't write more than 300 chars. Respond in only ONE paragraph. Be as concise as possible\"\n",
    "\n",
    "    println(\"LLM Response: \")\n",
    "    return ollamaChatModel.call(\n",
    "        Prompt(\n",
    "            SystemMessage(systemPrompt),\n",
    "            SystemMessage(\"Enriching data: $enrichedData\"),\n",
    "            UserMessage(\"User query: $query\")\n",
    "        )\n",
    "    ).result.output.text ?: \"\"\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing User Requests\n",
    "\n",
    "Let's test our processUserRequest function with a sample query:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:30.510456Z",
     "start_time": "2025-05-22T18:12:25.065407Z"
    }
   },
   "cell_type": "code",
   "source": "processUserRequest(\"What's trending right now?\", trendingTopicsHandler)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's trending right now\n",
      "trending_topics\n",
      "0.9956518411636353\n",
      "0.9\n",
      "\n",
      "What's trending right now\n",
      "trending_topics\n",
      "0.8692029714584351\n",
      "0.9\n",
      "\n",
      "[trending_topics]\n",
      "[[AI Tooling, AI Ethics, Mental Health, Generative Models, Emotional Intelligence, Open-Source Projects, Crypto, AI Automation, AI in Healthcare, DeFi, Anthropic, Video Creation, Content Generation, Multimodal Learning, Claude, These are the most mentioned topics. Don't try to guess what's being said in the topics.], \n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " The latest trends in AI discussions revolve around tooling for AI development, ethical considerations, mental health implications, and advancements in generative models like emotional intelligence and multimodal learning. Open-source projects are also a significant focus, showcasing collaborative efforts within the community. Beyond tech, cryptocurrency topics such as DeFi and Anthropic's involvement hint at broader financial and societal impacts. Moreover, AI is infiltrating sectors like healthcare through automation, transforming video creation with generative models, and even influencing content generation."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementing Summarization\n",
    "\n",
    "In addition to trending topics, we also want to be able to summarize posts about specific topics. For example, if a user asks \"What are people saying about Trump?\", we want to find posts about Trump and summarize them.\n",
    "\n",
    "First, let's define some example queries for the summarization route:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:30.566772Z",
     "start_time": "2025-05-22T18:12:30.518833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val summarizationRoute = listOf(\n",
    "    \"What are people saying about {topics}?\",\n",
    "    \"What’s the buzz around {topics}?\",\n",
    "    \"Any chatter about {topics}?\",\n",
    "    \"What are folks talking about regarding {topics}?\",\n",
    "    \"What’s being said about {topics} lately?\",\n",
    "    \"What have people been posting about {topics}?\",\n",
    "    \"What's trending in conversations about {topics}?\",\n",
    "    \"What’s the latest talk on {topics}?\",\n",
    "    \"Any recent posts about {topics}?\",\n",
    "    \"What's the sentiment around {topics}?\",\n",
    "    \"What are people saying about {topic1} and {topic2}?\",\n",
    "    \"What are folks talking about when it comes to {topic1}, {topic2}, or both?\",\n",
    "    \"What’s being said about {topic1}, {topic2}, and others?\",\n",
    "    \"Is there any discussion around {topic1} and {topic2}?\",\n",
    "    \"How are people reacting to both {topic1} and {topic2}?\",\n",
    "    \"What’s the conversation like around {topic1}, {topic2}, or related topics?\",\n",
    "    \"Are {topic1} and {topic2} being discussed together?\",\n",
    "    \"Any posts comparing {topic1} and {topic2}?\",\n",
    "    \"What's trending when it comes to {topic1} and {topic2}?\",\n",
    "    \"What are people saying about the relationship between {topic1} and {topic2}?\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:30.575810Z",
     "start_time": "2025-05-22T18:12:30.574411Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Storing Summarization Routes\n",
    "\n",
    "Now that we've defined our summarization routes, let's store them in our vector store:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:12:32.246214Z",
     "start_time": "2025-05-22T18:12:30.592500Z"
    }
   },
   "cell_type": "code",
   "source": "storeRouteDocumentsInRedis(\"summarization\", 0.8, summarizationRoute)",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementing the Summarization Function\n",
    "\n",
    "Now let's implement the summarization function. This function will:\n",
    "\n",
    "1. Extract topics from the user query using our topic modeling function\n",
    "2. For each topic, search for posts in Redis that have that topic\n",
    "3. Return the text of those posts\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:14:14.562930Z",
     "start_time": "2025-05-22T18:14:14.482810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import org.springframework.ai.chat.messages.SystemMessage\n",
    "import org.springframework.ai.chat.messages.UserMessage\n",
    "import org.springframework.ai.chat.prompt.Prompt\n",
    "\n",
    "fun summarization(userQuery: String): List<String> {\n",
    "    val existingTopics = jedisPooled.smembers(\"topics\").joinToString { \", \" }\n",
    "    val queryTopics = extractTopics(userQuery, existingTopics).replace(\"\\\"\", \"\").split(\", \")\n",
    "    println(queryTopics)\n",
    "\n",
    "    val posts = if (queryTopics.isEmpty()) {\n",
    "        val query = Query(\"*\")\n",
    "            .returnFields(\"text\")\n",
    "            .setSortBy(\"time_us\", false)\n",
    "            .dialect(2)\n",
    "            .limit(0, 10)\n",
    "\n",
    "        val result = jedisPooled.ftSearch(\n",
    "            \"postIdx\",\n",
    "            query\n",
    "        )\n",
    "\n",
    "        result.documents.map { document ->\n",
    "            document.get(\"text\").toString()\n",
    "        }\n",
    "    } else {\n",
    "        queryTopics.map { topic ->\n",
    "            val query = Query(\"@topics:{'$topic'}\")\n",
    "                .returnFields(\"text\")\n",
    "                .setSortBy(\"time_us\", false)\n",
    "                .dialect(2)\n",
    "                .limit(0, 10)\n",
    "\n",
    "            val result = jedisPooled.ftSearch(\n",
    "                \"postIdx\",\n",
    "                query\n",
    "            )\n",
    "\n",
    "            result.documents.map { document ->\n",
    "                document.get(\"text\").toString()\n",
    "            }\n",
    "        }.flatten()\n",
    "    }\n",
    "\n",
    "    return if (posts.isEmpty()) {\n",
    "        listOf(\"Nothing was found. Say that nothing was mentioned about these topics.\")\n",
    "    } else {\n",
    "        posts\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing the Summarization Function\n",
    "\n",
    "Let's test our summarization function with a sample query:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:15:09.957921Z",
     "start_time": "2025-05-22T18:15:08.766676Z"
    }
   },
   "cell_type": "code",
   "source": "summarization(\"What's being said about ChatGPT and Chatbots?\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ChatGPT, Chatbots, AI Conversational Agents]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Nothing was found. Say that nothing was mentioned about these topics.]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating a Multi-Handler\n",
    "\n",
    "Now that we have both trending topics and summarization handlers, let's create a combined handler that can handle both types of queries:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:15:15.402363Z",
     "start_time": "2025-05-22T18:15:15.363223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val multiHandler: (String, String) -> Iterable<String> = { route, query ->\n",
    "    when (route) {\n",
    "        \"trending_topics\" -> trendingTopics()\n",
    "        \"summarization\" -> summarization(query)\n",
    "        else -> emptyList()\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Testing the Complete System\n",
    "\n",
    "Now that we have our complete system, let's test it with different types of queries:\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:19:43.777643Z",
     "start_time": "2025-05-22T18:19:39.547121Z"
    }
   },
   "cell_type": "code",
   "source": "processUserRequest(\"What's being said about ChatGPT and Chatbots?\", multiHandler)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's being said about ChatGPT and Chatbots\n",
      "summarization\n",
      "0.8352869153022766\n",
      "0.8\n",
      "\n",
      "What's being said about ChatGPT and Chatbots\n",
      "summarization\n",
      "0.834100604057312\n",
      "0.8\n",
      "\n",
      "[summarization]\n",
      "[ ChatGPT, Chatbots, AI Conversations, Natural Language Processing]\n",
      "[[Nothing was found. Say that nothing was mentioned about these topics.], \n",
      "]\n",
      "LLM Response: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " I analyzed various posts discussing ChatGPT and chatbots, focusing on their capabilities, user experiences, ethical considerations, and future applications. The conversation highlighted the potential of AI-driven interactions to enhance customer service and automate tasks across industries. Users appreciated the efficiency and ease of use these tools offer, while concerns around privacy, data security, and ethical implications were also discussed. There's a general consensus that further development is crucial for refining their performance and addressing user trust issues."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:16:42.404433Z",
     "start_time": "2025-05-22T18:16:39.981899Z"
    }
   },
   "cell_type": "code",
   "source": "processUserRequest(\"What's trending now'?\", multiHandler)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's trending now'\n",
      "trending_topics\n",
      "0.9872353076934814\n",
      "0.9\n",
      "\n",
      "What's trending now'\n",
      "trending_topics\n",
      "0.9120936989784241\n",
      "0.9\n",
      "\n",
      "[trending_topics, summarization]\n",
      "[ “Trending, AI Trends, Technology Trends”]\n",
      "[[AI Tooling, AI Ethics, Mental Health, Generative Models, Emotional Intelligence, Open-Source Projects, Crypto, AI Automation, AI in Healthcare, DeFi, Anthropic, Video Creation, Content Generation, Multimodal Learning, Claude, These are the most mentioned topics. Don't try to guess what's being said in the topics.], [Nothing was found. Say that nothing was mentioned about these topics.], \n",
      "]\n",
      "LLM Response: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " Currently, AI tooling, ethical considerations in AI, mental health issues related to technology, generative models and emotional intelligence are hot topics on social media platforms, with mentions of Claude, Anthropic, multimodal learning, and open-source projects gaining traction. These areas explore how AI can be used responsibly and its impact on daily life, including healthcare, finance, video creation, content generation, and mental health support."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:20:04.817512Z",
     "start_time": "2025-05-22T18:20:04.527555Z"
    }
   },
   "cell_type": "code",
   "source": "processUserRequest(\"Isn't samba a great genre of music?\", multiHandler)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isn't samba a great genre of music\n",
      "summarization\n",
      "0.773983895778656\n",
      "0.8\n",
      "\n",
      "Isn't samba a great genre of music\n",
      "summarization\n",
      "0.772707998752594\n",
      "0.8\n",
      "\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sorry, I couldn't find any relevant information from your post. Try asking what's trending or what people are saying about a specific topic."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've built a simple question-answering system that can:\n",
    "\n",
    "1. Identify trending topics in posts\n",
    "2. Summarize posts about specific topics\n",
    "3. Route different types of user queries to the appropriate handler\n",
    "4. Generate natural language responses using a Large Language Model\n",
    "\n",
    "This demonstrates how to combine Redis, vector search, and LLMs to build an intelligent data analysis system. The system can be extended to handle more types of queries and to provide more detailed analysis of the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kotlin",
   "language": "kotlin",
   "name": "kotlin"
  },
  "language_info": {
   "name": "kotlin",
   "version": "1.9.23",
   "mimetype": "text/x-kotlin",
   "file_extension": ".kt",
   "pygments_lexer": "kotlin",
   "codemirror_mode": "text/x-kotlin",
   "nbconvert_exporter": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
